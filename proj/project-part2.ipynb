{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12ffa9b",
   "metadata": {},
   "source": [
    "<img src=\"data6.png\" style=\"width: 15%; float: right; padding: 1%; margin-right: 2%;\"/>\n",
    "\n",
    "# Final Project Part 2\n",
    "\n",
    "## Data 6, Fall 2024\n",
    "\n",
    "The saga continues! You can have up to one partner for the project (unless you were placed in a group of 3). Only one partner should submit on behalf of the entire group on gradescope (submit and add group member). This part of the project is due December 9th at 11:00 PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbffeafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /srv/conda/lib/python3.11/site-packages (1.55.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /srv/conda/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /srv/conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /srv/conda/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /srv/conda/lib/python3.11/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /srv/conda/lib/python3.11/site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: sniffio in /srv/conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /srv/conda/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /srv/conda/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /srv/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in /srv/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /srv/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /srv/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from IPython.display import YouTubeVideo, HTML, display\n",
    "from ipywidgets import interact, widgets\n",
    "%matplotlib inline\n",
    "%pip install openai #You may see an Error pop up, that's fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70805cb2-a883-4633-aa96-f873310e4861",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "# Section 0: Logistical Notes & Setup\n",
    "\n",
    "If you are working with a partner, you'll need some way of storing your intermediate work/sending work to one another. You have a few options:\n",
    "\n",
    "1. (RECOMMENDED): Type up your written responses on a separate Google Document, then transfer the work into this notebook.\n",
    "2. You can send the file to one another. The best way to do this is to send your notebook to one another by clicking File --> Download. Then, you can email that file to your partner, and have them download it on their end. Then, click on the Jupyter icon on the very top left of the screen. That will take you to your home directory. You can then access materials-fa24/proj and upload the file into that directory. **If you've generated any csv files, you can just download that directly as well.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105316c-bd21-4e9e-b4fe-d7bcec43751a",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 1 – Loading Your Data\n",
    "\n",
    "Go back to your work from Part 1 of the Project. From Question 7 of Part 1, `hand_coded_dataset` should be updated with at least two new columns (if you're working with a partner), with 20 rows.\n",
    "\n",
    "From there, run this line of code to turn your Table into a CSV file: `hand_coded_dataset.to_df().to_csv('hand_coded_data.csv', index = False)`.\n",
    "\n",
    "Once you've done so, click on the Jupyter Notebook icon on the top left of the page:\n",
    "\n",
    "<img src=\"jupyter.png\" style=\"width: 15%; float: center; padding: 1%; margin-right: 2%;\"/>\n",
    "\n",
    "From there, navigate to the `materials-fa24/proj` directory. You should see `hand_coded_data.csv` there now. Amazing!\n",
    "\n",
    "Now, load in your `hand_coded_dataset` as a Table, as well as your `full_dataset` containing the full 1000 rows that you chose from Part 1. **If you modified the original csv files, you'll have to re-download them [here](https://github.com/data-6-berkeley/materials-fa24/tree/main/proj).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251e7b7-b91b-4e0b-954c-ebf5bdb997c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_coded_dataset = Table.read_table(...) #LOAD IN YOUR DATA HERE\n",
    "full_dataset = Table.read_table(...) #LOAD IN YOUR DATA HERE\n",
    "tweet_column = ... #Note the name of your column containing the text data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d340077-9e05-4d7d-b875-d69f108256c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_coded_dataset #This should be a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f4dba-2c41-4eb2-8379-9f34279099f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_dataset.column(tweet_column)) #This should be a numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894022e-8f88-4470-9de5-45fd54519f3f",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "# Section 1: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's continue to get to know our dataset a little more. In Part 1, you did some qualitative analysis on the context of your data, but not much quantitative (numeric-based) analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7faece7-d070-422c-b175-26990e7506ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Section 1.5: Dictionary Review (Optional)\n",
    "\n",
    "If you're feeling shaky on dictionaries, feel free to dive in! Otherwise, proceed to Question 2 (you can hide this section by clicking on the triangle to the left of this cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca41043-b618-47d7-9f24-b8244b7371ff",
   "metadata": {},
   "source": [
    "\n",
    "In this project, we'll be taking a deep dive into a new Python data structure: **dictionaries**. While other data types we've seen in class are quite useful in many ways, dictionaries have a special purpose.\n",
    "\n",
    "<img src='./dictionary.png' width=300>\n",
    "\n",
    "**Dictionaries** can be very useful. They store key/value pairs that can be used to map one value to another. You can think of a dictionary as a list where the indexes (locations) of the values of the list are no longer their integer locations, but rather their keys.\n",
    "\n",
    ">In an array, you access the first item with `my_array.item(0)`.\n",
    "\n",
    ">In a dictionary, you access the \"key-th\" item with `my_dictionary[key]`.\n",
    "\n",
    "If we think of list items as having their \"address\" be their location in the list, then a dictionary value's \"address\" is its key.\n",
    "\n",
    "Some important properties of dictionaries to note:\n",
    "- The key and value **do not** have to be of the same type\n",
    "- We designate a new key/value entry in a dictionary in this format: *key* **:** *value*\n",
    "- We store all these key/value entries in a dictionaries with braces `{}` around the ends (like `[]` with a list) and commas separating the entries\n",
    "- Keys in a dictionary are unique, but values don't have to be unique. \n",
    "    - For example {'a': 100, 'a': 200} is not a valid dictionary\n",
    "    - but, {'a': 100, 'b': 100} is a valid dictionary\n",
    "    \n",
    "Let's take a closer look at a dictionary in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf5976-ab2a-46a6-9df3-13773ad4db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dictionary = {\"a\": 100, \"b\": 200, \"c\": 300}\n",
    "print(\"The value 'a' maps to the value:\", my_dictionary[\"a\"])\n",
    "print(\"The value 'b' maps to the value:\", my_dictionary[\"b\"])\n",
    "print(\"The value 'c' maps to the value:\", my_dictionary[\"c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea3aa58-d0a5-4a2a-a07f-2c77d2a77a6c",
   "metadata": {},
   "source": [
    "### How to Access the Data\n",
    "\n",
    "We can't access a dictionary's values like we can access a list's values. If we want the \"first\" item in a dictionary, we cannot ask for `my_dictionary[0]`, because this request is really asking \"What does the key 0 map to in this dictionary?\". If your dictionary does not have a value associated with the key 0, you will get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb65140-40b0-48c1-a71b-1b9616b79894",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "my_dictionary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae0e2f-cd42-4841-a088-c9678bc72e6d",
   "metadata": {},
   "source": [
    "A `KeyError` warning means that you asked for a key that is not in your dictionary. This may happen when you are writing a function with a dictionary, so if you see it, this is what it means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f4545-061b-4b3d-ad0b-17edec1bc7d9",
   "metadata": {},
   "source": [
    "### Changing the Data\n",
    "\n",
    "We can add the key value pair `(key, value)` with the following syntax:\n",
    "\n",
    "> `my_dictionary[key] = value`\n",
    "\n",
    "Run the cell below to change the `\"d\"` entry of the dictionary to 400:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ace6ef-c4de-4395-88b8-73f12cb4416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dictionary[\"d\"] = 400 # Add the key/value pair (\"d\", 400) to our dictionary\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7c835-1236-4f40-8a7e-a318f455e38c",
   "metadata": {},
   "source": [
    "We can use **any** data type we know as a value in a dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875dd32-361e-49f8-90f7-7c649beb0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the value we add is a list!\n",
    "my_dictionary[\"grocery list\"] = make_array(\"apples\", \"bananas\", \"carrots\")\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aeb503-b158-467c-ad8a-106109e6d764",
   "metadata": {},
   "source": [
    "...including even having a **dictionary itself** as a value! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5a4cf-54ff-4c16-8dda-2f8fff373e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dictionary[\"squares\"] = {1: 1, 2: 4, 3: 9, 4: 16}\n",
    "my_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a51d9-88ab-4059-bff8-54e5dcf70164",
   "metadata": {},
   "source": [
    "### Dictionary Iteration\n",
    "\n",
    "We can get a list of a dictionary's keys with the `.keys()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057edd1-0a44-44b8-a18a-bbc228796cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keys = my_dictionary.keys()\n",
    "my_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f9c6c-4866-4e14-9ac1-aaeafc7fde1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the type of this list of keys\n",
    "type(my_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ab551-c8fd-430d-902e-48ac1a184747",
   "metadata": {},
   "source": [
    "To iterate over the keys in a dictionary, we can use a `for` loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bb890-f454-4305-8f5d-25b68a096a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in my_dictionary:\n",
    "    print(\"I am a key, and my name is:\", key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8470922-1589-48ff-b959-191750a1e8aa",
   "metadata": {},
   "source": [
    "We can also get a list of a dictionary's values with the `.values()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649a0e8-6d90-4762-a73e-d0d0c6f6d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_values = my_dictionary.values()\n",
    "my_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba4a34-f322-4a87-aa66-cfee0f600bf5",
   "metadata": {},
   "source": [
    "We can iterate over the values of a dictionary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8cdade-1b7f-47e2-838d-8bc594e9a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in list(my_dictionary.values()):\n",
    "    print(\"I am a value, and my name is:\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5b7e4-eb34-444d-baa7-58f58d5df161",
   "metadata": {},
   "source": [
    "We can use this to do cool things like change all the values in a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03fa68-6b02-4483-8c85-50456932a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one_to_dictionary_values(dictionary):\n",
    "    for key in dictionary:\n",
    "        dictionary[key] = dictionary[key] + 1\n",
    "    return dictionary\n",
    "\n",
    "new_dictionary = {\"data\": 6, \"cs\": 61, \"poli sci\": 1}\n",
    "modified_dictionary = add_one_to_dictionary_values(new_dictionary)\n",
    "modified_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd2dd7f-cb5f-4ed5-af14-725ff3e56f66",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 2 - Word Frequency\n",
    "\n",
    "Let's start by making a dictionary of word frequencies. Using `full_dataset`, create a dictionary `word_counts` that maps all the unique words in all of the text to their corresponding frequencies. For instance, across the 1000 rows, if the word \"thanks\" shows up 50 times, we should have an entry in our dictionary with the key \"thanks\" and the corresponding value 50.\n",
    "\n",
    "Hint: Look into the String `split` method [here](https://data6.org/fa24/reference/#string-methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71853f0b-3a3b-4f4f-9ff7-7cafec4857ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "...\n",
    "\n",
    "#This sorts our dictionary based off the corresponding frequency, and turns it into a list.\n",
    "word_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f17ce0-cecc-473b-85bc-5c65bf6cb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[:20] #Takes the 20 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4afec-4ab5-42c8-9be2-d5a0f059989b",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 3 - Reflection\n",
    "\n",
    "Suppose we try using `word_counts` to get a sense of what people are talking about. In 1-2 sentences, what is a potential downside of this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834abad1-39ed-48bc-8b36-72ea72132121",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150e12a-5d6f-4cf9-9937-ba69c2e67f80",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 4 - TF-IDF\n",
    "\n",
    "To try to get more semantically significant terms (words that carry more meaning), let's compute a TF-IDF score for each word within a Tweet (when we mention Tweet, we're referring to each String containing multiple words within our dataset). TF-IDF stands for term frequency-inverse document frequency. In other words, we will count the frequency of some term (term frequency), and multiply it by the inverse of the number of times that same term appears across the entire dataset. Intuitively, this will give higher scores to words that appear multiple times in a given Tweet, but also don't appear that often across your entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f08db-56ae-4b6d-b60a-eb9ce2caab16",
   "metadata": {},
   "source": [
    "You'll start by computing the term frequency, which follows this formula:\n",
    "\n",
    "$$\n",
    "\\text{TF}(w, t) = \\frac{\\text{count}(w, t)}{\\text{total\\_words}(t)}\n",
    "$$\n",
    "\n",
    "**Description**:\n",
    "- `w`: The word being analyzed.\n",
    "- `t`: The specific Tweet you're examining.\n",
    "- `count(w, t)`: The number of occurrences of `w` in Tweet `t`.\n",
    "- `total_words(t)`: The total number of words in Tweet `t`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368391e1-55a3-4e41-9f78-25fad01ad475",
   "metadata": {},
   "source": [
    "Then, you'll calculate the inverse document frequency, following this formula:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + \\text{df}(w, d, c)} \\right)\n",
    "$$\n",
    "\n",
    "**Description**:\n",
    "- `w`: The word being analyzed.\n",
    "- `N`: The total number of Tweets in the dataset (in this case, 1000).\n",
    "- `df(w, d, c)`: The number of Tweets containing the word `w` in the entire dataset `d` when looking at column `c`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fe669-e4a9-4454-9510-d7d91384117d",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7967f-16a5-4e3c-b6a2-6bcdf0ba3346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count(w, t):\n",
    "    \"\"\"\n",
    "    Returns the number of times that the word w appears in the Tweet t\n",
    "\n",
    "    >>> word = \"hello\"\n",
    "    >>> tweet = \"hello there friend!\"\n",
    "    >>> count(word, tweet)\n",
    "    1\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "assert count(\"hello\", \"hello hello\") == 2\n",
    "assert count(\"hello\", \"hello\") == 1\n",
    "assert count(\"hello\", \"hello hello hello\\n \") == 3, \"Make sure that you handle newline characters properly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7af4-92fc-4165-9d8b-82df2f7a770f",
   "metadata": {},
   "source": [
    "## total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46e968-d089-4144-8b36-1e3c3f1bed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(t):\n",
    "    \"\"\"\n",
    "    Returns the number of words in the Tweet t, separated by whitespace.\n",
    "\n",
    "    >>> tweet = \"hello there friend!\"\n",
    "    >>> total_words(tweet)\n",
    "    3\n",
    "    \"\"\"\n",
    "    pass\n",
    "assert total_words(\"hello there friend!\") == 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bcd66c-87e4-43e1-b337-c2c4a1bdaece",
   "metadata": {},
   "source": [
    "## df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34bdae3-aee6-46be-8b69-fd02f2c37f56",
   "metadata": {},
   "source": [
    "In order to implement `df`, you'll want to use the `break` keyword. You can read more about it [here](https://www.geeksforgeeks.org/python-break-statement/#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6b7ae-924f-40ec-aa38-03729902622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I only want the first hi - hi hi hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a4d5d-f909-4c59-8f19-b194d4d09306",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in example.split():\n",
    "    if word == \"hi\":\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f48aa6-5a69-4d1d-a136-c8eda0304cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in example.split(): \n",
    "    if word == \"hi\":\n",
    "        print(word)\n",
    "        break #Notice \"hi\" is only printed once now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4d2da-af3b-43bf-b3ce-ed1191880d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(w, d, c):\n",
    "    \"\"\"\n",
    "    The number of Tweets containing the word w in the entire dataset d when looking at column c.\n",
    "\n",
    "    If the word \"hello\" appears in 3 tweets across d.column(c), return 3.\n",
    "\n",
    "    Example: \n",
    "    Tweet 1: \"hello hello\"\n",
    "    Tweet 2: \"Nope.\"\n",
    "    Tweet 3: \"Nah.\"\n",
    "    Tweet 4: \"hello there!\"\n",
    "    Tweet 5: \"hello\"\n",
    "    Tweet 6: \"hellooo\"\n",
    "\n",
    "    We expect 3 to be returned (Tweet 1, 4, and 5).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "test_table = Table().with_column(\"Text\", \n",
    "                                 make_array(\"hello hello\",\n",
    "                                            \"Nope.\",\n",
    "                                            \"Nah.\",\n",
    "                                            \"hello there!\",\n",
    "                                            \"hello\",\n",
    "                                            \"hellooo\"))\n",
    "assert df(\"hello\", test_table, \"Text\") == 3, \"Expected 3, but got: \" + str(df(\"hello\", test_table, \"Text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262c91f-cb78-4a6b-907c-28c2101b05e1",
   "metadata": {},
   "source": [
    "## tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fab439-b58c-40fe-803d-af48740a806b",
   "metadata": {},
   "source": [
    "Finally, fill in the function to compute the TF-IDF. As a reminder, here are all the formulas:\n",
    "\n",
    "$$\n",
    "\\text{TF}(w, t) = \\frac{\\text{count}(w, t)}{\\text{total\\_words}(t)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{IDF}(w, d, c) = \\log \\left( \\frac{N}{1 + \\text{df}(w, d, c)} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(w, t, d, c) = \\text{TF}(w, t) \\times \\text{IDF}(w, d, c)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fcc72-aad4-485b-bf79-52dbcf6f2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint: Use np.log to get the natural log of some number.\n",
    "\n",
    "def tf_idf(w, t, d, c):\n",
    "    \"\"\"\n",
    "    Compute the tf_idf score for word w within Tweet t.\n",
    "    To compute the inverse document frequency, use column c within dataset d.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c95c4e-0db6-444f-abbf-d75885e6be9e",
   "metadata": {},
   "source": [
    "## word_highest_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8323e1-8c8d-4d74-ba98-4396dcde9fe7",
   "metadata": {},
   "source": [
    "Finally, using `tf_idf`, begin by making a dictionary `word_highest_tf_idf` where the keys are each word in the dataset, and each value is a count of the number of times that particular word has the highest tf_idf score within that tweet.\n",
    "\n",
    "For example, if we have two tweets:\n",
    "\n",
    "\"Hi there friend\"\n",
    "\n",
    "and \n",
    "\n",
    "\"Hi friend\"\n",
    "\n",
    "with the following TF-IDF scores:\n",
    "\n",
    "`{\"Hi\": 1, \"there\": 0.5, \"friend\": 1}`\n",
    "\n",
    "`{\"Hi\": 0.3, \"friend\": 0.1}`\n",
    "\n",
    "we should expect the following dictionary assigned to `word_highest_tf_idf`:\n",
    "\n",
    "`{\"Hi\": 2, \"friend\": 1}`\n",
    "\n",
    "This is because \"Hi\" had the highest TF-IDF score in a tweet twice, and friend had the highest TF-IDF score in a tweet once.\n",
    "\n",
    "Note: We're using the `tqdm` library which gives us a progress bar, showing how long this process will take. This chunk of code took around 5 minutes to run for staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd12b02-bf20-4706-ab81-c558ec02cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_highest_tf_idf = {}\n",
    "\n",
    "# Iterate through each tweet in the dataset\n",
    "for text in tqdm(tweets, desc=\"Processing TF-IDF scores: \"):\n",
    "    word_scores = {}\n",
    "    for word in text.split():\n",
    "        score = ...\n",
    "        word_scores[word] = score\n",
    "    \n",
    "    max_score = max(word_scores.values()) #Calculate the maximum score\n",
    "    \n",
    "    for word, score in word_scores.items():\n",
    "        if score == max_score:\n",
    "            if word not in word_highest_tf_idf:\n",
    "                word_highest_tf_idf[word] = 1\n",
    "            else:\n",
    "                word_highest_tf_idf[word] += 1\n",
    "\n",
    "word_highest_tf_idf = sorted(word_highest_tf_idf.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451dc66f-d2d7-4a4e-b2d7-c13499d6bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_highest_tf_idf[:20] #Don't focus as much on the ones with only 1 as their value, as there are a lot of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa22ef-2dfb-4be8-b93b-31ad70b2be02",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 5 - TF-IDF Reflection\n",
    "\n",
    "How does the TF-IDF value compare to straight up word count in terms of usefulness? Any patterns you're noticing? 3-4 sentences is sufficient for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88484d6-ca39-4bc8-bb60-b48697455bcd",
   "metadata": {},
   "source": [
    "REPLACE THIS TEXT WITH YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c83d0-02e3-4c38-8b2b-76308971e843",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "# Section 2: Working with APIs\n",
    "\n",
    "An Application Programming Interface (API) allows programmers (people who write code) to access code that other people have written. One large benefit of using APIs is that you don't have to understand how a function is implemented, but just its functionality.\n",
    "\n",
    "An example is your usage of the Numpy module. When you call `np.average`, you can trust that the function will return the mean value of all the values in the Numpy array that was passed in, but you don't need to know exactly how the function is implemented!\n",
    "\n",
    "Most likely, you've used [chatgpt.com](https://chatgpt.com/) in order to access a Large Language Models (LLMs). But now, with our technical skills, we can use an API request to access these LLMs! \n",
    "\n",
    "To start off, we've shared your API key with you on [EdStem](https://edstem.org/us/courses/65093/discussion/5786092). This API key allows Open AI to recognize who is using their API, and charge us accordingly. Set your `API_KEY` to be a String of alphanumeric characters that we provide you.\n",
    "\n",
    "**IMPORTANT NOTE: Please DO NOT share your API key outside of this class. We will disable the API keys after the semester has ended, so if you'd like to play around with your code after the term, you'll have to get your own API key.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424f2a6-340a-4ef2-9722-dedd4198c631",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 6 - Set your API Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c37f6-2ff9-4365-9314-21956a3ee6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When writing code that will go to production (e.g. be seen by other people), \n",
    "you'll usually set your API key as an environment variable via your terminal.\n",
    "\"\"\"\n",
    "\n",
    "API_KEY = ... #Find the API key we've given you in EdStem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6fc2f-66d4-40b0-86dd-84a8f2e0ad6c",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 7 – Understanding the API Call\n",
    "## API Request Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd63c2-dc23-4e47-ac96-677d6030a686",
   "metadata": {},
   "source": [
    "Let's start by taking a look at the [chat completion](https://platform.openai.com/docs/api-reference/chat/create) functionality of these Open AI models. As you can see, you can think of an API request as just a fancy function call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b2651-cbf2-4eaa-9f38-091590eb5e25",
   "metadata": {},
   "source": [
    "When making a chat completion request, what is the data type of `messages` (e.g. integer, boolean, etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabec3b9-33ce-40ad-b26a-9065a84786db",
   "metadata": {},
   "source": [
    "REPLACE THIS TEXT WITH YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a865de3-d453-4b41-a5db-8f08f814f7f3",
   "metadata": {},
   "source": [
    "When making a chat completion request, what is the data type of each element within `messages` (e.g. integer, boolean, etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d61b5-1113-4eda-91f2-e169f51cca3b",
   "metadata": {},
   "source": [
    "REPLACE THIS TEXT WITH YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24ccee-24a3-437c-b259-dfc584e8f198",
   "metadata": {},
   "source": [
    "What are four different valid Strings that can be supplied as arguments in the `role` parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f8c2f-2305-4b3d-8603-48d54d8ec32a",
   "metadata": {},
   "source": [
    "REPLACE THIS TEXT WITH YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de9188-4cb6-4da1-810f-d8ecb11016ad",
   "metadata": {},
   "source": [
    "## API Request Return Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e87c1-a37f-40aa-899a-3ea0aad912e5",
   "metadata": {},
   "source": [
    "At its heart, an API request to the OpenAI API just requires some choice of LLM to be supplied as an argument, as well as the message we want to send to the LLM. The model is a String, and the message is a list of dictionaries. Each dictionary contains a mapping of \"role\" to the corresponding role, and \"content\" to the actual message. Let's start by making our first API request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e3dde-b91d-4e6a-b9cf-ce66592fc3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2896f-fc44-46d1-80ee-6cc3a5040da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi there!\"} #We create a list of dictionaries containing the prompt.\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\", #Picking the model \n",
    "  messages=message #Supplying the message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb11ba-dca7-4c32-b327-394016cd2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(completion) #Run this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527b059-2e99-45b2-b70f-522640e74fa6",
   "metadata": {},
   "source": [
    "Notice that a [chat completion object](https://platform.openai.com/docs/api-reference/chat/object) is returned from the API call. \n",
    "\n",
    "We can then convert this into json so we can parse the outputs! Alternatively, the API reference also shows you how to access it via **dot notation**, which is not in scope for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57119c9d-4a0f-4d66-89e0-285b853292b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = completion.json() \n",
    "response_dict = json.loads(response_json)\n",
    "type(response_dict) #Notice that this is a dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce349206-c0ea-47f2-be25-a3cba496584b",
   "metadata": {},
   "source": [
    "Super cool! We turned a chat completion object into a dictionary, which we know how to parse. Let's take a look inside the dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c2440-151f-4a9d-903b-59de931b83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a818c0-6169-418d-99b6-4327b739a17c",
   "metadata": {},
   "source": [
    "Given `response_dict`, write an expression that would evaluate to the response from the LLM (in this case, \"Hello! How can I assist you today?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38bd71-606d-41e2-bfe8-d95665aa0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR EXPRESSION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e264bc1-357e-4782-9ac5-9af1a91e99f4",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 8 – Making Your Own API Request!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233dcc-fff5-41a2-a4c2-c946d36d14a5",
   "metadata": {},
   "source": [
    "Try modifying and making your own API request to see what the best restaurant in Berkeley is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea9674-b920-4fac-b01e-d5e97e6b6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The system content allows us to specify external instructions to the model.\n",
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"Please keep your response brief.\"},\n",
    "    {\"role\": \"user\", \"content\": ...} #YOUR PROMPT HERE\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\", #Picking model \n",
    "  messages=message #Supplying the message\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content) #Accesses the response from the model using dot notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05a95f-810b-474e-b191-bae1dbca732c",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 9 – Creating A Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f24ce9-94ef-48ad-9972-37a2ebad01d8",
   "metadata": {},
   "source": [
    "Since writing the same lines of code over and over can be cumbersome, let's write a utility function to help us make these API requests and process them.\n",
    "\n",
    "Fill in `prompt`, which takes in a String representing the model we want to use, as well as a list of dictionaries representing the message we want to send to the model. This function will return the message the LLM sends back to us, since we don't care about the other parts of the chat completion object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bc1e0-9e85-405f-af76-d030b078450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(model, message):\n",
    "    \"\"\"\n",
    "    Makes an API request to a LLM and returns the message.\n",
    "\n",
    "    Inputs:\n",
    "    model: A String representing the model we're using\n",
    "    message: A list of dictionaries, where each dictionary has two entries.\n",
    "    The first entry in each dictionary should have the key \"role\", and a value representing the role.\n",
    "    The second entry in each dictionary should have the key \"content\", and a value representing the content.\n",
    "\n",
    "    Returns:\n",
    "    A String representing the result of an API request to the model, parsed to just get the message back.\n",
    "\n",
    "    >>> model_choice = \"gpt-4o-mini\"\n",
    "    >>> message = [{\"role\": \"user\", \"content\": \"Hi there!\"}]\n",
    "    >>> prompt(model_choice, message)\n",
    "    'Hello! How can I assist you today?'\n",
    "    \"\"\"\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5c2e9-499b-47d0-b5c8-9ea0151fc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test your implementation of prompt here\n",
    "\n",
    "model_choice = \"gpt-4o-mini\"\n",
    "message = [{\"role\": \"user\", \"content\": \"Hi there!\"}]\n",
    "prompt(model_choice, message) #Should show something along the lines of \"How can I assist you today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc2b97-32f6-4d22-ba72-6aac13dd7a5c",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 10 - Preparing the Model Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74fde7-eb88-493c-a165-a0ffc157b7b0",
   "metadata": {},
   "source": [
    "## Model Context - Codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cbe9d5-149b-4794-9624-76215d3e19b7",
   "metadata": {},
   "source": [
    "Now, let's attempt to use the gpt-4o-mini model to label all of our Tweets! To begin, take your codebook from Part 1 and turn it into a dictionary, with the Code as the key, and the description as a value. For instance, if we had the following codebook: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f884d62-b7bd-421d-b97f-49d15c6af9c6",
   "metadata": {},
   "source": [
    "| General Category    | Description                                       | Values                          | Code |\n",
    "|---------------------|---------------------------------------------------|---------------------------------|------|\n",
    "| **Emotion**         | Expresses happiness or generally positive emotion. | Happiness                      | EM-H |\n",
    "|                     | Expresses a feeling of sorrow or unhappiness.      | Sadness                        | EM-S |\n",
    "|                     | Expresses a feeling of displeasure or rage.        | Anger                          | EM-A |\n",
    "|                     | Expresses astonishment or unexpected reaction.     | Surprise                       | EM-SP |\n",
    "|                     | Expresses fear or anxiety.                         | Fear                           | EM-F |\n",
    "| **Intensity**       | Indicates a low amount of emotional strength.        | Low                            | IN-L |\n",
    "|                     | Indicates a moderate level of emotional strength.  | Medium                         | IN-M |\n",
    "|                     | Indicates a strong emotional reaction.             | High                           | IN-H |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e36c3a-203c-40eb-a26b-c5a019a3d866",
   "metadata": {},
   "source": [
    "That would turn into the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec9693-9505-4ce6-b8e0-634771adf4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_codebook = {\n",
    "    \"EM-H\": \"Expresses happiness or generally positive emotion.\",\n",
    "    \"EM-S\": \"Expresses a feeling of sorrow or unhappiness.\",\n",
    "    \"EM-A\": \"Expresses a feeling of displeasure or rage.\",\n",
    "    \"EM-SP\": \"Expresses astonishment or unexpected reaction.\",\n",
    "    \"EM-F\": \"Expresses fear or anxiety.\",\n",
    "\n",
    "    \"IN-L\": \"Indicates a low amount of emotional strength.\",\n",
    "    \"IN-M\": \"Indicates a moderate level of emotional strength.\",\n",
    "    \"IN-H\": \"Indicates a strong emotional reaction.\"\n",
    "}\n",
    "example_codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de9d20-cece-48d6-b7de-4b5a67440bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUT YOUR CODEBOOK AS A DICTIONARY HERE\n",
    "codebook = {\n",
    "    ...\n",
    "}\n",
    "codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5a74d-73e0-4014-9171-eecb325a8e02",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 11 – Developing our prompt\n",
    "Now that you've gotten a chance to turn your codebook into a dictionary (which can be type casted into a String), try using `prompt` to have \"gpt-4o-mini\" code your 20 Tweets in the `hand_coded_dataset`! Once you've done so, assign this to a new `hand_model_coded_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30247d-23a8-4e05-a316-a992d4e6aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coded_responses = make_array()\n",
    "\n",
    "for ...:\n",
    "    message = [\n",
    "    {\"role\": \"system\", \"content\": \"Please apply codes according to this codebook:\" + str(codebook)},\n",
    "    {\"role\": \"system\", \"content\": \"Here is the Tweet: \" + ...} #ONLY SUPPLY THE TWEET TEXT. Do not attempt any prompt engineering yet.\n",
    "        ]\n",
    "    llm_response = prompt(..., message)\n",
    "    model_coded_responses = ...\n",
    "\n",
    "hand_model_coded_dataset = hand_coded_dataset.with_column(\"LLM response 1\", model_coded_responses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a968ab-5722-49b2-a7db-555aa4c1273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_model_coded_dataset.show(20) #Run this to compare your hand-coded responses with the LLM response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e84147-a074-4dae-81f7-902a8ad5cdad",
   "metadata": {},
   "source": [
    "Are there any differences between how you coded your responses and how the model coded the responses? What about the formatting of the codes? 2-3 sentences should be sufficient here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8148f2-605a-4bef-bde9-d95b2452ad8a",
   "metadata": {},
   "source": [
    "REPLACE THIS TEXT WITH YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fcfb1-59b5-415b-aab6-060757374672",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 12 – Prompt Engineering\n",
    "Let's make sure that our prompt has a bit more structure to it as well. \n",
    "\n",
    "1. Follow the [LLM prompting guidelines](https://direct.mit.edu/view-large/figure/4722326/coli_a_00502_i004.tif) to make your prompt better!\n",
    "2. Then, read over the [Open AI guide](https://platform.openai.com/docs/guides/prompt-engineering#tactic-provide-examples) on how to do few-shot prompting.\n",
    "\n",
    "Your updated prompt should:\n",
    "1. Include a brief context (15 words or less) \n",
    "2. Enumerate options as alphabetical multiple choice, separated by a new line. For this, you can include the newline character \"\\n\" in your prompt.\n",
    "3. Specify constraints\n",
    "4. Specify actions under uncertainty\n",
    "5. Provide 2 examples of expected inputs and outputs\n",
    "\n",
    "Use this to add a new column, `LLM response 2`, to the `hand_model_coded_dataset` Table. \n",
    "\n",
    "Note: The two references seem to disagree as to whether the context should be provided at the very end (as the Open AI guide suggests), or at the beginning (as the MIT paper suggests). I'll leave that choice up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed868427-6881-4692-ac57-3af1c7e4253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coded_responses = make_array()\n",
    "\n",
    "...\n",
    "\n",
    "hand_model_coded_dataset = hand_model_coded_dataset.with_column(\"LLM response 2\", model_coded_responses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7972a-dc87-4ee0-b5f6-684fa8c7bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_model_coded_dataset.show(20) #Run this to compare your hand-coded responses with both LLM response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db792fc0-acb6-47ec-a8c9-90d37cac0f02",
   "metadata": {},
   "source": [
    "Are there any improvements from the initial prompt? What about the formatting of the codes? 2-3 sentence should be sufficient here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13b628-28bb-499e-9326-d370c89da1a6",
   "metadata": {},
   "source": [
    "REPLACE THIS TEXT WITH YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b3e30-d43d-474a-83f5-9bc663bee2bd",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 13 – Coding the Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fa6e4-e6e2-45be-9530-8f0cbbd457fb",
   "metadata": {},
   "source": [
    "Once you're satisfied with your prompt and how your LLM is performing, use that same prompt from Question 12 to code the entire `full_dataset`. We've provided a progress checking library so you can see how long it will take (warning: it will likely take ~10 minutes per model).\n",
    "\n",
    "Once you're done, fill in a new table with the following data (you can add more columns if you wish):\n",
    "\n",
    "- One column contains the original Tweet\n",
    "- The remaining columns of the resulting table should comprise the codes from the codebook you made.\n",
    "- Each value in the code columns should be filled with either True or False depending on whether that Tweet had that code applied to it or not.\n",
    "\n",
    "For example, if we had the Tweet \"I really like Data 6!\" and `gpt-4o-mini` applied the codes \"EM-H EM-A\" out of the codebook containing the codes EM-H, EM-A, EM-S, and EM-B, the resulting table `gpt-4o-codes` would look like this:\n",
    "\n",
    "\n",
    "| Text    | EM-H                                       | EM-A                          | EM-S | EM-B |\n",
    "|---------------------|---------------------------------------------------|---------------------------------|------| ------|\n",
    "| I really like Data 6!         | True | True                      | False | False |\n",
    "\n",
    "\n",
    "\n",
    "For the easiest parsing, have the model return the codes applied to a given Tweet as a space separated list. Otherwise, you're welcome to look into the String [strip method](https://www.w3schools.com/python/ref_string_strip.asp) to handle any weirdness with whitespace.\n",
    "\n",
    "**Be careful making assumptions about what exactly the LLM will return to you. Make sure you have some way of gracefully handling codes that don't exist or are formatted incorrectly without erroring everything out. One potential solution is to check if that specific code is in the Table's column labels.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48efccf-5308-48ba-b72d-3ab71c4be5ba",
   "metadata": {},
   "source": [
    "## gpt-4o-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0fec19-37c3-41aa-b280-143c524c251d",
   "metadata": {},
   "source": [
    "For this cell, use the model \"gpt-4o-mini\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fc954-f55a-4b1c-a0c6-1ece7e064141",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_codes = ...\n",
    "\n",
    "for text in tqdm(full_dataset.column(tweet_column), desc=\"Processing text\"):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed19982-da13-4665-806e-c03e8e0509dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc460c-cba9-4375-9c44-67091c16ab39",
   "metadata": {},
   "source": [
    "## gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5558da-9d04-4fef-8ff8-6bf6e7dcacbc",
   "metadata": {},
   "source": [
    "For this cell, use the model \"gpt-3.5-turbo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758886bf-c20d-4155-96d9-fe3d9c21bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_turbo_codes = ...\n",
    "\n",
    "for text in tqdm(full_dataset.column(tweet_column), desc=\"Processing text\"):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cead57-09e2-4cd8-9f03-b1f0c6fea0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_turbo_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3ba83-e621-4e90-9864-fd703ad4397e",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 14 - Joining the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5241b3c-6e8e-41bf-bfd3-276126b9f6ae",
   "metadata": {},
   "source": [
    "Now, we should have three tables - the original `full_dataset` and two tables, `gpt_4o_codes` and `gpt_turbo_codes`. It'll be helpful to know both which codes were applied, as well as the data from the `full_dataset`.\n",
    "\n",
    "For instance, if our `gpt_4o_codes` table looked like this:\n",
    "\n",
    "| Text    | EM-H                                       | EM-A                          | EM-S | EM-B |\n",
    "|---------------------|---------------------------------------------------|---------------------------------|------| ------|\n",
    "| I really like Data 6!         | True | True                      | False | False |\n",
    "\n",
    "and our `full_dataset` looked like this:\n",
    "\n",
    "| Text    | Username                                      | Likes                          |\n",
    "|---------------------|---------------------------------------------------|---------------------------------|\n",
    "| I really like Data 6!         | Jedi | 50                      |\n",
    "\n",
    "Join together the two tables to make `gpt_4o_joined` and `gpt_turbo_joined` respectively. For instance, `gpt_4o_joined` should contain the following data (order of columns does not matter):\n",
    "\n",
    "| Text    | EM-H                                       | EM-A         | EM-S | EM-B | Username           | Likes        |\n",
    "|---------------------|---------------------------------------------------|---------------------------------|------| ------|------| ------|\n",
    "| I really like Data 6!         | True | True                      | False | False | Jedi | 50 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd4f374-f549-4fc2-8111-b4c2f368fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_joined = ... #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209d32a-258f-40b3-960f-c60c278d67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_turbo_joined = ... #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3566b-7f07-4270-89f9-6e1fa33e9a34",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "# Section 3: Visualizations and Results\n",
    "\n",
    "<br></br>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 15 – Exploring Your Findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8022483-8192-40a0-ad98-ad3744f7d78c",
   "metadata": {},
   "source": [
    "This is the most open ended aspect of the project! There are only a few requirements:\n",
    "\n",
    "1. Use the outputs of the TF-IDF dictionary to describe specific references people are making that are significant (feel free to do additional external research if you'd like). Those of you using the airline tweets dataset, the word counts dictionary may end up being more useful than TF-IDF (so you're welcome to use that instead).\n",
    "2. Generate at least two plots that involve your CSS concept as one variable (e.g. the codes applied from gpt-4o-mini), choosing visualizations that are appropriate for the type of variable you're using.\n",
    "3. Each plot is paired with some sort of analysis (3-4 sentences per visual) explaining the relationship (or lack thereof) that we're observing. It is OK to have visualizations that do not have a clear correlation! That's data for you :)\n",
    "\n",
    "\n",
    "Here are a few ideas:\n",
    "1. All of the datasets also contain other columns that you could explore. For example, is there a relationship between the CSS concepts that you've applied to a particular tweet, and the number of likes it received?\n",
    "2. Is there any difference between the codes applied by hand vs by gpt-4o-mini? Or between gpt-3.5-turbo and gpt-4o-mini?\n",
    "3. Are there any relationships within certain codes in your codebook? For example, examining if Tweets that were tagged with an anger emotion generally had higher levels of emotional intensity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf431b3-f37f-4189-a496-d95bf99c73f1",
   "metadata": {},
   "source": [
    "Create as many cells as you'd like to. This is your time to run wild!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3c9ac-9759-48fb-8d1c-a987b6834a1b",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Once you're done, go to File --> Save and Export Notebook as --> PDF, and submit to Gradescope! Make sure that all the visualizations are showing in the resulting PDF, and all cells have been run. Congrats on finishing Part 2 of the Final Project! Woohooo!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
